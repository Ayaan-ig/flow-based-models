{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ntEvoEhqTgQe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import math\n",
        "from sys import exit\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as functional\n",
        "\n",
        "torch.set_default_dtype(torch.float64) #use double precision numbers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine_Coupling(nn.Module):\n",
        "    def __init__(self, mask, hidden_dim):\n",
        "        super(Affine_Coupling, self).__init__()\n",
        "        self.input_dim = len(mask)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        ## mask to seperate positions that do not change and positions that change.\n",
        "        ## mask[i] = 1 means the ith position does not change.\n",
        "        self.mask = nn.Parameter(mask, requires_grad = False)\n",
        "\n",
        "        ## layers used to compute scale in affine transformation\n",
        "        self.scale_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        self.scale_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.scale_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
        "        self.scale = nn.Parameter(torch.Tensor(self.input_dim))\n",
        "        init.normal_(self.scale)\n",
        "\n",
        "        ## layers used to compute translation in affine transformation\n",
        "        self.translation_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
        "        self.translation_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.translation_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
        "\n",
        "    def _compute_scale(self, x):\n",
        "        ## compute scaling factor using unchanged part of x with a neural network\n",
        "        s = torch.relu(self.scale_fc1(x*self.mask))\n",
        "        s = torch.relu(self.scale_fc2(s))\n",
        "        s = torch.relu(self.scale_fc3(s)) * self.scale\n",
        "        return s\n",
        "\n",
        "    def _compute_translation(self, x):\n",
        "        ## compute translation using unchanged part of x with a neural network\n",
        "        t = torch.relu(self.translation_fc1(x*self.mask))\n",
        "        t = torch.relu(self.translation_fc2(t))\n",
        "        t = self.translation_fc3(t)\n",
        "        return t\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## convert latent space variable to observed variable\n",
        "        s = self._compute_scale(x)\n",
        "        t = self._compute_translation(x)\n",
        "\n",
        "        y = self.mask*x + (1-self.mask)*(x*torch.exp(s) + t)\n",
        "        logdet = torch.sum((1 - self.mask)*s, -1)\n",
        "\n",
        "        return y, logdet\n",
        "\n",
        "    def inverse(self, y):\n",
        "        ## convert observed varible to latent space variable\n",
        "        s = self._compute_scale(y)\n",
        "        t = self._compute_translation(y)\n",
        "\n",
        "        x = self.mask*y + (1-self.mask)*((y - t)*torch.exp(-s))\n",
        "        logdet = torch.sum((1 - self.mask)*(-s), -1)\n",
        "\n",
        "        return x, logdet"
      ],
      "metadata": {
        "id": "39vNhw1XTmra"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RealNVP_2D(nn.Module):\n",
        "    '''\n",
        "    A vanilla RealNVP class for modeling 2 dimensional distributions\n",
        "    '''\n",
        "    def __init__(self, masks, hidden_dim):\n",
        "        '''\n",
        "        initialized with a list of masks. each mask define an affine coupling layer\n",
        "        '''\n",
        "        super(RealNVP_2D, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.masks = nn.ParameterList(\n",
        "            [nn.Parameter(torch.Tensor(m),requires_grad = False)\n",
        "             for m in masks])\n",
        "\n",
        "        self.affine_couplings = nn.ModuleList(\n",
        "            [Affine_Coupling(self.masks[i], self.hidden_dim)\n",
        "             for i in range(len(self.masks))])\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## convert latent space variables into observed variables\n",
        "        y = x\n",
        "        logdet_tot = 0\n",
        "        for i in range(len(self.affine_couplings)):\n",
        "            y, logdet = self.affine_couplings[i](y)\n",
        "            logdet_tot = logdet_tot + logdet\n",
        "\n",
        "        ## a normalization layer is added such that the observed variables is within\n",
        "        ## the range of [-4, 4].\n",
        "        logdet = torch.sum(torch.log(torch.abs(4*(1-(torch.tanh(y))**2))), -1)\n",
        "        y = 4*torch.tanh(y)\n",
        "        logdet_tot = logdet_tot + logdet\n",
        "\n",
        "        return y, logdet_tot\n",
        "\n",
        "    def inverse(self, y):\n",
        "        ## convert observed variables into latent space variables\n",
        "        x = y\n",
        "        logdet_tot = 0\n",
        "\n",
        "        # inverse the normalization layer\n",
        "        logdet = torch.sum(torch.log(torch.abs(1.0/4.0* 1/(1-(x/4)**2))), -1)\n",
        "        x  = 0.5*torch.log((1+x/4)/(1-x/4))\n",
        "        logdet_tot = logdet_tot + logdet\n",
        "\n",
        "        ## inverse affine coupling layers\n",
        "        for i in range(len(self.affine_couplings)-1, -1, -1):\n",
        "            x, logdet = self.affine_couplings[i].inverse(x)\n",
        "            logdet_tot = logdet_tot + logdet\n",
        "\n",
        "        return x, logdet_tot"
      ],
      "metadata": {
        "id": "_NrhLHpITu7Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Masks used to define the number and the type of affine coupling layers\n",
        "## In each mask, 1 means that the variable at the correspoding position is\n",
        "## kept fixed in the affine couling layer\n",
        "masks = [[1.0, 0.0],\n",
        "         [0.0, 1.0],\n",
        "         [1.0, 0.0],\n",
        "         [0.0, 1.0],\n",
        "         [1.0, 0.0],\n",
        "         [0.0, 1.0],\n",
        "         [1.0, 0.0],\n",
        "         [0.0, 1.0]]\n",
        "\n",
        "## dimenstion of hidden units used in scale and translation transformation\n",
        "hidden_dim = 128\n",
        "\n",
        "## construct the RealNVP_2D object\n",
        "realNVP = RealNVP_2D(masks, hidden_dim)\n",
        "if torch.cuda.device_count():\n",
        "    realNVP = realNVP.cuda()\n",
        "device = next(realNVP.parameters()).device\n",
        "\n",
        "optimizer = optim.Adam(realNVP.parameters(), lr = 0.0001)\n",
        "\n",
        "num_steps = 5000\n",
        "# num_steps = 5000\n",
        "\n",
        "## the following loop learns the RealNVP_2D model by data\n",
        "## in each loop, data is dynamically sampled from the scipy moon dataset\n",
        "for idx_step in range(num_steps):\n",
        "    ## sample data from the scipy moon dataset\n",
        "    X, label = datasets.make_moons(n_samples = 512, noise = 0.05)\n",
        "    X = torch.Tensor(X).to(device = device)\n",
        "\n",
        "    ## transform data X to latent space Z\n",
        "    z, logdet = realNVP.inverse(X)\n",
        "\n",
        "    ## calculate the negative loglikelihood of X\n",
        "    loss = torch.log(z.new_tensor([2*math.pi])) + torch.mean(torch.sum(0.5*z**2, -1) - logdet)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (idx_step + 1) % 1000 == 0:\n",
        "        print(f\"idx_steps: {idx_step:}, loss: {loss.item():.5f}\")\n",
        "\n",
        "## after learning, we can test if the model can transform\n",
        "## the moon data distribution into the normal distribution\n",
        "X, label = datasets.make_moons(n_samples = 1000, noise = 0.05)\n",
        "X = torch.Tensor(X).to(device = device)\n",
        "z, logdet_jacobian = realNVP.inverse(X)\n",
        "z = z.cpu().detach().numpy()\n",
        "\n",
        "X = X.cpu().detach().numpy()\n",
        "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
        "fig.clf()\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(X[label==0,0], X[label==0,1], \".\")\n",
        "plt.plot(X[label==1,0], X[label==1,1], \".\")\n",
        "plt.title(\"X sampled from Moon dataset\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(z[label==0,0], z[label==0,1], \".\")\n",
        "plt.plot(z[label==1,0], z[label==1,1], \".\")\n",
        "plt.title(\"Z transformed from X\")\n",
        "plt.xlabel(r\"$z_1$\")\n",
        "plt.ylabel(r\"$z_2$\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "owViqzCBTwpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## after learning, we can also test if the model can transform\n",
        "## the normal distribution into the moon data distribution\n",
        "z = torch.normal(0, 1, size = (1000, 2)).to(device = device)\n",
        "X, _ = realNVP(z)\n",
        "X = X.cpu().detach().numpy()\n",
        "z = z.cpu().detach().numpy()\n",
        "\n",
        "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
        "fig.clf()\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(z[:,0], z[:,1], \".\")\n",
        "plt.title(\"Z sampled from normal distribution\")\n",
        "plt.xlabel(r\"$z_1$\")\n",
        "plt.ylabel(r\"$z_2$\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(X[:,0], X[:,1], \".\")\n",
        "plt.title(\"X transformed from Z\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yqQnKdsKT4I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJteKR6SVeYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}